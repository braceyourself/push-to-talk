---
phase: 01-mode-rename-and-live-voice-session
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - live_session.py
  - personality/core.md
  - personality/voice-style.md
  - personality/context.md
  - push-to-talk.py
  - indicator.py
autonomous: false

must_haves:
  truths:
    - "User can select 'Live (Voice Conversation)' in the AI Mode combo box in Settings"
    - "Selecting Live mode auto-starts a WebSocket connection to OpenAI Realtime API"
    - "User can hold PTT keys, speak, release, and hear AI respond through speakers"
    - "Conversation context persists across multiple PTT presses within a session"
    - "Switching away from Live mode cleanly disconnects the session"
    - "AI personality is loaded from personality/*.md files"
    - "Overlay widget shows colored dot (green=listening, blue=speaking, gray=idle) and status text"
    - "Session reconnects on next PTT press after idle timeout"
  artifacts:
    - path: "live_session.py"
      provides: "LiveSession class with WebSocket, ConversationState, personality loading"
      min_lines: 200
    - path: "personality/core.md"
      provides: "Base personality traits and behavioral rules"
      min_lines: 15
    - path: "personality/voice-style.md"
      provides: "Voice-specific response rules (concise, fillers OK, no markdown)"
      min_lines: 10
    - path: "personality/context.md"
      provides: "Auto-managed session context placeholder"
      min_lines: 3
    - path: "push-to-talk.py"
      provides: "Live mode routing in on_press, session lifecycle methods, config watcher"
    - path: "indicator.py"
      provides: "AI mode combo entry, LiveOverlayWidget class, overlay management"
  key_links:
    - from: "push-to-talk.py"
      to: "live_session.py"
      via: "import and instantiation of LiveSession"
      pattern: "from live_session import LiveSession"
    - from: "live_session.py"
      to: "personality/"
      via: "_build_personality reads *.md files"
      pattern: "personality_dir.*glob.*md"
    - from: "push-to-talk.py"
      to: "indicator.py"
      via: "config.json ai_mode='live' triggers overlay show"
      pattern: "ai_mode.*live"
    - from: "live_session.py"
      to: "OpenAI Realtime API"
      via: "WebSocket at wss://api.openai.com/v1/realtime"
      pattern: "websockets.connect.*realtime"
---

<objective>
Build the new "live" AI mode: a LiveSession class that connects to the OpenAI Realtime API for voice conversation, a multi-file personality system, session lifecycle management in push-to-talk.py, and a floating overlay widget in indicator.py. Users hold PTT to speak, release to send, and hear AI respond — with conversation memory across turns.

Purpose: This is the core Phase 1 deliverable — real-time voice conversation that will later (Phase 3) gain task orchestration capabilities.
Output: `live_session.py` with LiveSession + ConversationState, `personality/` directory with 3 markdown files, updated `push-to-talk.py` with live mode routing and config watcher, updated `indicator.py` with overlay widget and AI mode combo entry.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-mode-rename-and-live-voice-session/01-RESEARCH.md
@.planning/phases/01-mode-rename-and-live-voice-session/01-CONTEXT.md
@.planning/phases/01-mode-rename-and-live-voice-session/01-01-SUMMARY.md
@openai_realtime.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LiveSession class and personality system</name>
  <files>live_session.py, personality/core.md, personality/voice-style.md, personality/context.md</files>
  <action>
**Create `personality/core.md`** with the AI's identity and behavioral rules, based on CONTEXT.md decisions and clawdbot patterns from research:
- Name: Not given a name — it's "the assistant" (keep it utilitarian)
- Personality: Direct, opinionated, dry humor and wit
- Task-oriented memory — connects dots between topics mentioned earlier
- Friendly but efficient, stays on topic, gets to the point
- When asked for opinions, gives them confidently (doesn't hedge)
- Contradiction handling: newest info wins, explicit > implicit
- These files are the AI's memory across sessions — each session wakes fresh and loads these

**Create `personality/voice-style.md`** with voice-specific rules:
- Concise responses — a few sentences max, never paragraphs
- Fillers are allowed and encouraged ("hmm", "let me think", "right") — they signal listening
- No markdown, no bullet points, no code blocks — this is spoken language
- No emoji, no asterisks, no formatting of any kind
- Numbers spoken naturally ("twenty-three" not "23")
- Technical terms are fine but explain jargon if it comes up naturally
- Don't announce actions ("I'll help you with that") — just do it
- Don't ask "is there anything else?" — let the conversation flow naturally

**Create `personality/context.md`** as a placeholder:
```markdown
# Session Context
No previous session context available. This is a fresh start.
```
This file will be auto-managed in future phases for session continuity.

**Create `live_session.py`** — a new file modeled on the existing `openai_realtime.py` patterns but purpose-built for voice conversation without tools:

```python
class ConversationState:
    """Tracks conversation turns for context management."""
    def __init__(self):
        self.history = []       # list of {role, item_id, text}
        self.summary_count = 0
        self.latest_tokens = 0
        self.summarizing = False

class LiveSession:
    """OpenAI Realtime voice session with personality and memory."""
```

Key implementation details:

1. **Constructor** `__init__(self, api_key, voice="ash", on_status=None, on_audio_level=None)`:
   - Store api_key, voice, callbacks
   - `self.ws = None`, `self.running = False`, `self.audio_player = None`
   - `self.playing_audio = False`, `self.audio_done_time = 0`
   - `self.conversation = ConversationState()`
   - `self.personality_prompt = self._build_personality()`
   - `self._idle_timer = None`, `self._idle_timeout = 120` (2 minutes)

2. **`_build_personality(self)`**: Load all `*.md` files from `personality/` directory (sorted alphabetically), concatenate with `\n\n`. The directory is `Path(__file__).parent / "personality"`.

3. **`async connect(self)`**: Connect to `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17` (use the same model as existing code for now — GA model `gpt-realtime` can be updated later if needed). Send `session.update` with:
   - `modalities: ["text", "audio"]`
   - `instructions`: the personality_prompt
   - `voice`: self.voice
   - `input_audio_format: "pcm16"`, `output_audio_format: "pcm16"`
   - `input_audio_transcription: {"model": "whisper-1"}`
   - `turn_detection: {"type": "semantic_vad", "eagerness": "medium", "interrupt_response": true}`
   - `tools: []`, `tool_choice: "none"` (no tools in Phase 1)

4. **`async disconnect(self)`**: Same pattern as RealtimeSession — close ws, kill audio_player, unmute mic via `pactl set-source-mute @DEFAULT_SOURCE@ 0`.

5. **`start_audio_player(self)`**: Same as RealtimeSession — `aplay -r 24000 -f S16_LE -t raw -q`.

6. **`async send_audio(self, audio_data)`**: Base64 encode, send as `input_audio_buffer.append`. Same as existing.

7. **`async handle_events(self)`**: Event loop processing — async for message in self.ws. Handle these events:
   - `response.audio.delta`: Play audio, mute mic, set status "speaking", reset idle timer
   - `response.audio_transcript.delta`: Print text for logging
   - `response.audio_transcript.done`: Print newline, log transcript
   - `response.done`: Track conversation turn in `self.conversation.history` (extract `response.usage.total_tokens` into `self.conversation.latest_tokens`), unmute mic after delay (use `response.audio.done` event if available, else 1.0s delay after `response.done`), call `maybe_summarize()`, set status "listening", start idle timer
   - `input_audio_buffer.speech_started`: Unmute mic, set status "listening"
   - `input_audio_buffer.speech_stopped`: Mute mic, set status "speaking" (AI will respond)
   - `conversation.item.input_audio_transcription.completed`: Track user turn in conversation.history with item_id and text
   - `error`: Log and set status "error"
   - `session.created`: Log session ID
   - Connection closed: Set status "disconnected"

8. **`async record_and_send(self)`**: Same pattern as RealtimeSession.record_and_send() — pw-record, read chunks, skip while playing_audio.

9. **`async maybe_summarize(self)`**: If `self.conversation.latest_tokens > 20000` and not already summarizing, summarize all but last 3 turns via `openai.OpenAI().chat.completions.create()` with `gpt-4o-mini`. Inject summary as system message via `conversation.item.create` at root position. Delete summarized items via `conversation.item.delete`. Update local history. Use try/finally to clear summarizing flag.

10. **`_reset_idle_timer(self)`**: Cancel existing timer, start new one. After `self._idle_timeout` seconds of no activity, set `self.running = False` (triggers graceful disconnect). Use `asyncio.get_event_loop().call_later()`.

11. **`async run(self)`**: Set running=True, connect, start_audio_player, set status "listening", run gather(handle_events, record_and_send). On exception: status "error". Finally: disconnect, status "idle".

12. **`async seed_context(self)`**: Called after connect. If conversation.history has items, create a summary message and inject via `conversation.item.create`. This enables session reconnection with context.

13. **`stop(self)`**: Set running=False (same as RealtimeSession).

14. **`request_interrupt(self)`**: Thread-safe interrupt flag (same as RealtimeSession).

Import block: `os, json, base64, asyncio, subprocess, time, pathlib.Path`. Conditionally import `websockets` and `openai`.

Constants: `SAMPLE_RATE = 24000`, `CHANNELS = 1`, `CHUNK_SIZE = 4096`, `REALTIME_URL`, `SUMMARY_TRIGGER = 20000`, `KEEP_LAST_TURNS = 3`.
  </action>
  <verify>
```bash
cd /home/ethan/code/push-to-talk
# Syntax check
python3 -c "import py_compile; py_compile.compile('live_session.py'); print('OK')"
# Verify personality files exist
ls personality/core.md personality/voice-style.md personality/context.md
# Verify key classes exist
python3 -c "from live_session import LiveSession, ConversationState; print('Imports OK')"
# Verify personality loading
python3 -c "
from live_session import LiveSession
# Just test _build_personality without connecting
import unittest.mock as mock
with mock.patch.object(LiveSession, '__init__', lambda self: None):
    ls = LiveSession()
    ls.personality_prompt = ''
    from pathlib import Path
    personality_dir = Path('personality')
    parts = [f.read_text() for f in sorted(personality_dir.glob('*.md'))]
    print(f'Loaded {len(parts)} personality files, total {sum(len(p) for p in parts)} chars')
"
```
  </verify>
  <done>
`live_session.py` exists with LiveSession and ConversationState classes. Personality directory has 3 markdown files. LiveSession loads personality from files, manages WebSocket lifecycle, tracks conversation state, and supports idle timeout and context summarization.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire live mode into push-to-talk.py and indicator.py</name>
  <files>push-to-talk.py, indicator.py</files>
  <action>
**In `push-to-talk.py`:**

1. **Import LiveSession** (near line 99, alongside RealtimeSession import):
```python
try:
    from live_session import LiveSession
    LIVE_SESSION_AVAILABLE = True
except ImportError:
    LIVE_SESSION_AVAILABLE = False
```

2. **PushToTalk.__init__** (around line 722): Add instance variables:
```python
# Live voice session
self.live_session = None
self.live_thread = None
self.config_watcher_running = True
self._last_config_mtime = 0
```
Start the config watcher thread at the end of __init__:
```python
# Start config watcher for live mode auto-start
self.config_watcher_thread = threading.Thread(target=self._watch_config, daemon=True)
self.config_watcher_thread.start()
```

3. **`_watch_config(self)`**: New method. Polls config.json mtime every 500ms. When mtime changes, reload config. If `ai_mode` changed to `"live"` and no active session, call `start_live_session()`. If changed away from `"live"` and session active, call `stop_live_session()`.
```python
def _watch_config(self):
    """Watch config.json for ai_mode changes to auto-start/stop live session."""
    import os
    config_path = CONFIG_FILE
    while self.config_watcher_running:
        try:
            mtime = os.path.getmtime(config_path)
            if mtime != self._last_config_mtime:
                self._last_config_mtime = mtime
                new_config = load_config()
                old_ai_mode = self.config.get('ai_mode', 'claude')
                new_ai_mode = new_config.get('ai_mode', 'claude')
                self.config = new_config
                if new_ai_mode == 'live' and old_ai_mode != 'live':
                    if not self.live_session:
                        print("Config watcher: Live mode selected, auto-starting session", flush=True)
                        self.start_live_session()
                elif old_ai_mode == 'live' and new_ai_mode != 'live':
                    if self.live_session:
                        print("Config watcher: Left live mode, stopping session", flush=True)
                        self.stop_live_session()
        except Exception as e:
            pass  # Don't crash the watcher
        time.sleep(0.5)
```

4. **`start_live_session(self)`**: New method. Pattern matches `start_realtime_session()`:
```python
def start_live_session(self):
    """Start a live voice conversation session."""
    if not LIVE_SESSION_AVAILABLE:
        print("ERROR: live_session module not available", flush=True)
        set_status('error')
        return False

    api_key = get_openai_api_key()
    if not api_key:
        print("No OpenAI API key found for live session", flush=True)
        prompt_api_key()
        return False

    voice = self.config.get('openai_voice', 'ash')
    self.live_session = LiveSession(
        api_key=api_key,
        voice=voice,
        on_status=set_status
    )

    def run_session():
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self.live_session.run())
        finally:
            loop.close()
            self.live_session = None

    self.live_thread = threading.Thread(target=run_session, daemon=True)
    self.live_thread.start()
    return True
```

5. **`stop_live_session(self)`**: New method:
```python
def stop_live_session(self):
    """Stop the live voice session cleanly."""
    if self.live_session:
        self.live_session.stop()
        self.live_session = None
    if self.live_thread:
        self.live_thread.join(timeout=2)
        self.live_thread = None
    subprocess.run(['pactl', 'set-source-mute', '@DEFAULT_SOURCE@', '0'],
                   capture_output=True)
    set_status('idle')
```

6. **`on_press` handler** (around line 2100): Add a new `elif` for `ai_mode == 'live'`:
```python
elif ai_mode == 'live':
    if not self.live_session:
        # Session not running (timed out or first start) — reconnect
        self.start_live_session()
    # Live session handles audio via its own record_and_send loop
    # PTT press in live mode is a no-op for recording since session auto-listens
```
This goes after the `elif ai_mode == 'conversation':` block and before the `elif ai_mode == 'realtime':` block.

7. **`on_release` handler**: No changes needed for live mode since the session manages its own audio pipeline (not PTT record/release pattern). The AI hotkey toggle behavior already exists for realtime — live mode uses config watcher instead.

8. **Voice commands for "live mode"** — In the voice command section (around where "go live" used to be, which Plan 01 renamed to "dictate mode"), add NEW voice commands to switch AI mode to live:
```python
if text_lower in ['live mode', 'go live', 'going live']:
    print(f"Voice command: switching to live AI mode", flush=True)
    self.config['ai_mode'] = 'live'
    save_config(self.config)
    subprocess.Popen(['notify-send', '-t', '2000', 'Push-to-Talk', 'Live mode activated'],
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    set_status('success')
    return
```
This fulfills RENAME-03: "live mode" voice command now activates the new live AI mode.

**In `indicator.py`:**

9. **AI Mode combo box** (line 314, in `create_general_tab`): Add a new entry AFTER the existing ones:
```python
self.ai_mode_combo.append("live", "Live (Voice Conversation)")
```
Place it between "realtime" and "interview" entries, or at the top just after "claude" — use judgment on ordering. Recommended order: Claude, Live, Conversation, Realtime, Interview.

10. **`on_ai_mode_changed` handler** (around line 801): No structural changes needed — it already saves `ai_mode` to config, and the config watcher in push-to-talk.py will pick up the change. However, add show/hide logic for a live options section (placeholder for now — can be empty box):
```python
# Show/hide live options (placeholder for future settings)
is_live = active == 'live'
```

11. **AI mode display names** in PopupWindow (line 1033): Add `'live': 'Live (Voice)'` to the `ai_displays` dict.

12. **LiveOverlayWidget class**: Create a new GTK3 window class in `indicator.py`. This is a floating, draggable, transparent widget:
```python
class LiveOverlayWidget(Gtk.Window):
    """Floating overlay showing live session status."""
```
Key features:
- `Gtk.WindowType.TOPLEVEL`, undecorated, keep_above, skip_taskbar, accept_focus=False
- Type hint: `Gdk.WindowTypeHint.DOCK`
- RGBA visual for transparency
- Size: 180x44 pixels
- Contains: colored status dot (12px radius) + status text ("Listening", "Speaking", "Idle", "Disconnected")
- Dot colors: green=#4ade80 (listening), blue=#60a5fa (speaking), gray=#6b7280 (idle/disconnected)
- Dark semi-transparent background: rgba(30, 30, 30, 0.85)
- Rounded corners via Cairo clip
- Draggable: track mouse press/motion events for move
- Position persistence: save x,y to config as `live_overlay_x`, `live_overlay_y`; load on creation
- Default position: bottom-right of screen, 20px margin
- `update_status(self, status)` method — called from set_status via GLib.idle_add
- `show()`/`hide()` controlled by live session start/stop

This replaces the standard status dot while live mode is active. When live mode ends, hide the overlay and the regular dot resumes.

The overlay does NOT need waveform visualization for this plan — keep it simple: dot + text. Waveform is Claude's discretion per CONTEXT.md and can be added later.

13. **Wire overlay lifecycle**: In the main indicator startup code, create the overlay instance (hidden). Expose a function like `show_live_overlay()` / `hide_live_overlay()` / `update_live_overlay(status)` that push-to-talk.py can call (via the set_status mechanism or a new file-based signal). Simplest approach: when `set_status` writes to the status file, the indicator's poll loop checks if live mode is active and routes updates to the overlay.
  </action>
  <verify>
```bash
cd /home/ethan/code/push-to-talk
# Syntax check both files
python3 -c "import py_compile; py_compile.compile('push-to-talk.py'); py_compile.compile('indicator.py'); print('OK')"
# Verify LiveSession import exists
grep "from live_session import" push-to-talk.py
# Verify live mode in AI combo
grep "live.*Voice Conversation" indicator.py
# Verify config watcher
grep "_watch_config" push-to-talk.py
# Verify overlay class exists
grep "class LiveOverlayWidget" indicator.py
# Verify live mode in on_press
grep "ai_mode == 'live'" push-to-talk.py
# Verify voice command for live mode
grep "'live mode'" push-to-talk.py
```
  </verify>
  <done>
Live mode fully wired: AI mode combo shows "Live (Voice Conversation)". Config watcher auto-starts/stops session on mode change. on_press handles live mode PTT. Voice commands "live mode" / "go live" switch AI mode. LiveOverlayWidget shows status dot + text during live sessions. Overlay is draggable with position persistence.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete live voice session mode: LiveSession with OpenAI Realtime API, personality system (3 markdown files), config watcher for auto-start, overlay widget, and all UI/voice command wiring.
  </what-built>
  <how-to-verify>
1. **Settings UI**: Right-click the status dot > Settings. Verify "Live (Voice Conversation)" appears in the AI Mode dropdown.
2. **Auto-start**: Select "Live (Voice Conversation)" in the AI Mode dropdown. The session should auto-connect. Check journal logs: `journalctl --user -u push-to-talk -f`
3. **Overlay**: A floating widget should appear showing a colored dot and status text. It should be draggable.
4. **Voice conversation**: Hold Right Ctrl + Right Shift, speak a sentence, release. You should hear the AI respond through speakers within 1-2 seconds. The overlay should show "Listening" (green) while you speak and "Speaking" (blue) while AI responds.
5. **Memory persistence**: After the AI responds, hold PTT again and reference something from the previous exchange (e.g., "you just said..."). The AI should remember.
6. **Mode switch teardown**: Switch AI mode to "Claude + Whisper" in Settings. The overlay should disappear, the session should disconnect cleanly. Check logs for clean shutdown.
7. **Personality**: The AI's responses should be concise, direct, and natural-sounding (not robotic or overly formal).
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. `python3 -c "import py_compile; py_compile.compile('push-to-talk.py'); py_compile.compile('indicator.py'); py_compile.compile('live_session.py'); print('All OK')"` — no syntax errors
2. `ls personality/core.md personality/voice-style.md personality/context.md` — all three personality files exist
3. `grep "class LiveSession" live_session.py` — class exists
4. `grep "class ConversationState" live_session.py` — class exists
5. `grep "class LiveOverlayWidget" indicator.py` — overlay class exists
6. `grep "ai_mode.*live" indicator.py push-to-talk.py` — live mode wired in both files
7. `grep "semantic_vad" live_session.py` — using semantic VAD not server_vad
8. `grep "tools.*\[\]" live_session.py` — no tools in Phase 1
</verification>

<success_criteria>
- User can select "Live (Voice Conversation)" in Settings and a Realtime API session auto-starts
- Holding PTT and speaking results in AI voice response through speakers
- Multiple PTT presses maintain conversation context
- Switching away from Live mode cleanly disconnects
- Overlay widget shows live session status with colored dot
- Personality is loaded from personality/*.md files
- No zombie processes or stuck audio after session ends
</success_criteria>

<output>
After completion, create `.planning/phases/01-mode-rename-and-live-voice-session/01-02-SUMMARY.md`
</output>
