---
phase: 05-barge-in
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - live_session.py
autonomous: true

must_haves:
  truths:
    - "AI's conversation context includes annotation showing where the user interrupted and what was not spoken"
    - "After barge-in, the system immediately starts listening (no wait for fade to finish)"
    - "Post-interrupt silence threshold is shortened so the AI responds faster to quick corrections"
    - "AI receives context annotation with spoken vs unspoken text before generating next response"
  artifacts:
    - path: "live_session.py"
      provides: "Sentence tracking, interruption annotation, shortened post-interrupt silence, overlay pulse"
      contains: "_spoken_sentences"
  key_links:
    - from: "_read_cli_response"
      to: "_spoken_sentences"
      via: "Tracks sentences sent to TTS at three flush sites: (1) normal streaming sentence loop, (2) post-tool-buffer flush, (3) final sentence_buffer flush"
      pattern: "_spoken_sentences"
    - from: "_trigger_barge_in"
      to: "_barge_in_annotation"
      via: "Builds annotation from spoken vs unspoken sentences and stores for next turn"
      pattern: "_barge_in_annotation"
    - from: "_llm_stage"
      to: "_barge_in_annotation"
      via: "Prepends stored annotation to the next user message before sending to CLI"
      pattern: "_barge_in_annotation"
    - from: "_stt_stage"
      to: "SILENCE_DURATION"
      via: "Uses shortened silence threshold after barge-in for faster response"
      pattern: "_post_barge_in"
---

<objective>
Add the intelligence layer on top of the barge-in mechanism: track which sentences the AI spoke vs. which were interrupted, annotate the conversation context so the AI knows it was cut off, shorten the post-interrupt silence threshold for faster response, and add a brief visual pulse on the overlay.

Purpose: This completes BARGE-04 — interrupted speech is properly annotated in context so the AI can adapt. It also makes the recovery flow feel natural with faster response after interruption.
Output: Modified `live_session.py` with sentence tracking, context annotation, and post-interrupt tuning.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-barge-in/05-CONTEXT.md
@.planning/phases/05-barge-in/05-RESEARCH.md
@.planning/phases/05-barge-in/05-01-SUMMARY.md
@live_session.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Track spoken vs unspoken sentences and send interruption annotation to CLI</name>
  <files>live_session.py</files>
  <action>
**Sentence tracking state in `__init__`:**
- Add `self._spoken_sentences = []` — list of sentence strings sent to TTS in current turn
- Add `self._played_sentence_count = 0` — how many sentences had their audio fully played
- Add `self._full_response_text = ""` — full LLM response text for the current turn
- Add `self._was_interrupted = False` — flag set by `_trigger_barge_in`, cleared at start of new turn
- Add `self._barge_in_annotation = None` — stored annotation prepended to next user message

**In `_read_cli_response()` — reset tracking at method start:**

At the top of the method (after `gen_id = self.generation_id` at line ~1016), reset:
```python
self._spoken_sentences = []
self._played_sentence_count = 0
self._full_response_text = ""
self._was_interrupted = False
```

**In `_read_cli_response()` — track sentences at ALL THREE flush sites:**

There are exactly three locations where sentences are put onto `_llm_out_q` as `FrameType.TEXT_DELTA`. Add `self._spoken_sentences.append(clean)` after the `await self._llm_out_q.put(...)` call at each site:

**Site 1 — Normal streaming sentence loop (lines ~1131-1139):**
Inside the `while True` / `SENTENCE_END_RE.search(sentence_buffer)` loop. After:
```python
if clean:
    await self._llm_out_q.put(PipelineFrame(
        type=FrameType.TEXT_DELTA,
        generation_id=gen_id,
        data=clean
    ))
```
Add: `self._spoken_sentences.append(clean)`

**Site 2 — Post-tool-buffer flush (lines ~1154-1161):**
After the `if saw_tool_use and post_tool_buffer.strip()` block that does:
```python
clean = self._strip_markdown(post_tool_buffer)
if clean:
    await self._llm_out_q.put(PipelineFrame(
        type=FrameType.TEXT_DELTA,
        generation_id=gen_id,
        data=clean
    ))
```
Add: `self._spoken_sentences.append(clean)`

**Site 3 — Final sentence_buffer flush (lines ~1164-1171):**
After the `if not saw_tool_use and sentence_buffer.strip()` block that does:
```python
clean = self._strip_markdown(sentence_buffer)
if clean:
    await self._llm_out_q.put(PipelineFrame(
        type=FrameType.TEXT_DELTA,
        generation_id=gen_id,
        data=clean
    ))
```
Add: `self._spoken_sentences.append(clean)`

**In `_tts_stage()` — emit sentence boundary sentinels:**

In `_tts_stage()` (lines ~1472-1531), after each sentence's TTS audio has been fully streamed (after the `await process.wait()` at line ~1529, but still inside the per-sentence processing block):
```python
# After process.wait() — mark sentence boundary for playback stage
await self._audio_out_q.put(PipelineFrame(
    type=FrameType.CONTROL,
    generation_id=gen_id,
    data="sentence_done"
))
```

**In `_playback_stage()` — count completed sentences:**

In `_playback_stage()` (lines ~1535-1612), add handling for CONTROL sentinel frames. After the `frame.generation_id != self.generation_id` check (line ~1557) and before the END_OF_TURN check (line ~1560):
```python
if frame.type == FrameType.CONTROL and frame.data == "sentence_done":
    self._played_sentence_count += 1
    continue
```

**In `_trigger_barge_in()` — build and store annotation:**

After the existing barge-in logic (from Plan 01), before setting status to "listening", add:

```python
# Build interruption annotation
self._was_interrupted = True
spoken = self._spoken_sentences[:self._played_sentence_count]
unspoken = self._spoken_sentences[self._played_sentence_count:]
spoken_text = " ".join(spoken) if spoken else "(nothing)"
unspoken_text = " ".join(unspoken) if unspoken else "(nothing)"

self._barge_in_annotation = (
    f"[The user interrupted you. "
    f"They heard up to: \"{spoken_text}\". "
    f"Your unspoken response was: \"{unspoken_text}\". "
    f"Adjust based on what the user says next.]"
)
print(f"Barge-in: Annotation ({self._played_sentence_count}/{len(self._spoken_sentences)} sentences spoken)", flush=True)
```

**In `_llm_stage()` — consume annotation on next user message:**

In `_llm_stage()` (line ~1430-1431), after building `user_content` from task_context and transcript:
```python
# Build user message with ambient task context
task_context = self._build_task_context()
user_content = f"{task_context}\n\n{transcript}" if task_context else transcript
```

Add immediately after:
```python
# Prepend barge-in annotation if the AI was interrupted
if self._barge_in_annotation:
    user_content = f"{self._barge_in_annotation}\n\n{user_content}"
    self._barge_in_annotation = None
```

This ensures the annotation is delivered WITH the user's next utterance, not as a separate message that would trigger an unwanted CLI response.
  </action>
  <verify>
1. `grep -n "_spoken_sentences" live_session.py` shows tracking in __init__, reset in _read_cli_response, appended at 3 sites, and consumed in _trigger_barge_in
2. `grep -n "_played_sentence_count" live_session.py` shows init, reset, increment in playback stage, and read in trigger
3. `grep -n "sentence_done" live_session.py` shows sentinel put in TTS stage and handled in playback stage
4. `grep -n "_barge_in_annotation" live_session.py` shows init, set in trigger, consumed in LLM stage
5. `python3 -c "import live_session; print('OK')"` — module imports without error
  </verify>
  <done>Sentences sent to TTS are tracked at all three flush sites in _read_cli_response. Playback stage counts completed sentences via sentinel frames from TTS stage. On barge-in, an annotation describing spoken vs unspoken text is stored and prepended to the user's next message, giving the AI context about the interruption.</done>
</task>

<task type="auto">
  <name>Task 2: Shorten post-interrupt silence threshold and add overlay pulse</name>
  <files>live_session.py</files>
  <action>
**Shortened post-interrupt silence threshold in `_stt_stage()`:**

Add state tracking:
- `self._post_barge_in = False` in `__init__` — set True by `_trigger_barge_in()`, cleared after the next transcription completes

In `_trigger_barge_in()`, add: `self._post_barge_in = True`

In `_stt_stage()`, replace the single `SILENCE_DURATION = 0.8` constant (line ~1265) with two constants:
```python
SILENCE_DURATION_NORMAL = 0.8
SILENCE_DURATION_POST_BARGE = 0.4  # Faster response after interruption
```

In the silence detection check (line ~1353), where it currently compares:
```python
elif time.time() - silence_start > SILENCE_DURATION and has_speech and len(audio_buffer) > int(SAMPLE_RATE * MIN_BUFFER_SECONDS * BYTES_PER_SAMPLE):
```

Replace `SILENCE_DURATION` with a dynamic value:
```python
current_silence_duration = SILENCE_DURATION_POST_BARGE if self._post_barge_in else SILENCE_DURATION_NORMAL
```

Use `current_silence_duration` instead of `SILENCE_DURATION` in the comparison. Place the variable computation just before the comparison (inside the `if rms < SILENCE_THRESHOLD:` block, before `elif`).

After a successful transcription is sent to `_stt_out_q` (line ~1376-1378, after the `await self._stt_out_q.put(PipelineFrame(type=FrameType.TRANSCRIPT, ...))` call):
```python
self._post_barge_in = False
```

Also clear it after the flush-on-mute transcription path (line ~1311-1315, after the TRANSCRIPT frame is sent):
```python
self._post_barge_in = False
```

**Overlay pulse on barge-in:**

Per CONTEXT.md: "Brief visual pulse/flash on the overlay when barge-in activates — momentary acknowledgment, then transitions to listening state (no new overlay state)."

The transition from "speaking" to "listening" IS the visual acknowledgment. The `_trigger_barge_in()` method (from Plan 01) already sets `self._set_status("listening")`. No additional overlay state needed — the status change is sufficient.

**Enhanced barge-in logging:**
In `_trigger_barge_in()`, update the `_log_event` call:
```python
self._log_event("barge_in",
    spoken_sentences=self._played_sentence_count,
    total_sentences=len(self._spoken_sentences),
    cooldown_until=self._barge_in_cooldown_until
)
```
  </action>
  <verify>
1. `grep -n "SILENCE_DURATION_POST_BARGE\|SILENCE_DURATION_NORMAL" live_session.py` shows both constants
2. `grep -n "_post_barge_in" live_session.py` shows flag in __init__, set in trigger, cleared after transcription, checked in silence detection
3. `grep -n "current_silence_duration" live_session.py` shows dynamic threshold usage
4. `python3 -c "import live_session; print('OK')"` — module imports without error
  </verify>
  <done>Post-interrupt silence threshold is halved (0.4s vs 0.8s) for one turn after barge-in, so the AI responds faster to quick corrections. Overlay transitions naturally from "speaking" to "listening" on barge-in. Barge-in events are logged with sentence tracking metadata.</done>
</task>

</tasks>

<verification>
1. Module imports: `python3 -c "from live_session import LiveSession; print('OK')"`
2. Sentence tracking: `_spoken_sentences` populated at 3 sites in `_read_cli_response`, consumed in `_trigger_barge_in`
3. Sentence counting: `sentence_done` sentinel in TTS stage, counted in playback stage
4. Annotation: `_barge_in_annotation` stored on barge-in, prepended to next user message in LLM stage
5. Dynamic silence: `SILENCE_DURATION_POST_BARGE` used when `_post_barge_in` is True
6. Full flow: User speaks during AI playback -> VAD detects ~0.5s speech -> barge-in triggers -> queues drain -> trailing filler plays -> annotation stored -> status changes to listening -> user speaks -> shortened silence threshold -> transcription -> annotation prepended -> AI responds knowing it was interrupted
</verification>

<success_criteria>
- Interrupted AI response text is annotated with spoken vs. unspoken portions
- Annotation is prepended to the user's next message (not sent as a separate turn)
- Post-interrupt silence threshold is 0.4s (vs normal 0.8s) for one turn
- AI can adapt its response based on knowing it was interrupted
- Status transitions cleanly from "speaking" to "listening" on barge-in
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/05-barge-in/05-02-SUMMARY.md`
</output>
