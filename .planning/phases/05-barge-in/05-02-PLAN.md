---
phase: 05-barge-in
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - live_session.py
autonomous: true

must_haves:
  truths:
    - "AI's conversation context includes annotation showing where the user interrupted and what was not spoken"
    - "After barge-in, the system immediately starts listening (no wait for fade to finish)"
    - "Post-interrupt silence threshold is shortened so the AI responds faster to quick corrections"
    - "Conversation continues naturally after interruption — AI knows it was cut off"
  artifacts:
    - path: "live_session.py"
      provides: "Sentence tracking, interruption annotation, shortened post-interrupt silence, overlay pulse"
      contains: "_spoken_sentences"
  key_links:
    - from: "_read_cli_response"
      to: "_spoken_sentences"
      via: "Tracks sentences sent to TTS and counts which ones had their audio fully played"
      pattern: "_spoken_sentences"
    - from: "_trigger_barge_in"
      to: "_send_to_cli"
      via: "Sends interruption annotation message to CLI with spoken vs unspoken text"
      pattern: "interrupted"
    - from: "_stt_stage"
      to: "SILENCE_DURATION"
      via: "Uses shortened silence threshold after barge-in for faster response"
      pattern: "_post_barge_in"
---

<objective>
Add the intelligence layer on top of the barge-in mechanism: track which sentences the AI spoke vs. which were interrupted, annotate the conversation context so the AI knows it was cut off, shorten the post-interrupt silence threshold for faster response, and add a brief visual pulse on the overlay.

Purpose: This completes BARGE-04 — interrupted speech is properly annotated in context so the AI can adapt. It also makes the recovery flow feel natural with faster response after interruption.
Output: Modified `live_session.py` with sentence tracking, context annotation, and post-interrupt tuning.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-barge-in/05-CONTEXT.md
@.planning/phases/05-barge-in/05-RESEARCH.md
@.planning/phases/05-barge-in/05-01-SUMMARY.md
@live_session.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Track spoken vs unspoken sentences and send interruption annotation to CLI</name>
  <files>live_session.py</files>
  <action>
**Sentence tracking state in `__init__`:**
- Add `self._spoken_sentences = []` — list of sentence strings sent to TTS in current turn
- Add `self._played_sentence_count = 0` — how many sentences had their audio fully played
- Add `self._full_response_text = ""` — full LLM response text for the current turn
- Add `self._was_interrupted = False` — flag set by `_trigger_barge_in`, cleared at start of new turn

**In `_read_cli_response()` — track sentences sent to TTS:**

At the start of the method (after `gen_id = self.generation_id`), reset sentence tracking:
```python
self._spoken_sentences = []
self._played_sentence_count = 0
self._full_response_text = ""
self._was_interrupted = False
```

Where sentences are flushed to TTS (the `await self._llm_out_q.put(PipelineFrame(type=FrameType.TEXT_DELTA, ...))` calls for complete sentences in the normal streaming path), also append to tracking:
```python
self._spoken_sentences.append(clean)
```

Do the same for the post-tool-buffer flush and the final sentence_buffer flush at the end.

**In `_playback_stage()` — count played sentences:**

We need a lightweight way to know how many sentences fully played. The simplest approach: count END_OF_TURN frames' arrival to mark "all played," and count TTS_AUDIO byte accumulation.

Actually, a simpler approach that's precise enough: track cumulative bytes sent to TTS per sentence, and track cumulative bytes played. Then on barge-in, we can estimate which sentence was being spoken.

Even simpler (per CONTEXT.md: "imprecise but sufficient"): In `_playback_stage`, maintain a counter of TTS_AUDIO frames played. Since each sentence generates a variable number of frames, we need the TTS stage to insert sentence-boundary markers.

**Simplest viable approach:** In `_tts_stage()`, after finishing TTS for a sentence (after the `await process.wait()` in the per-sentence Piper loop), put a sentinel frame to mark the sentence boundary:

```python
# After the while loop that streams Piper output for one sentence:
await self._audio_out_q.put(PipelineFrame(
    type=FrameType.CONTROL,
    generation_id=gen_id,
    data="sentence_done"
))
```

In `_playback_stage()`, when receiving a CONTROL frame with data "sentence_done":
```python
if frame.type == FrameType.CONTROL and frame.data == "sentence_done":
    self._played_sentence_count += 1
    continue
```

**In `_trigger_barge_in()` — send annotation to CLI:**

After the existing barge-in logic (from Plan 01), add context annotation:

```python
# Build interruption annotation
self._was_interrupted = True
spoken = self._spoken_sentences[:self._played_sentence_count]
unspoken = self._spoken_sentences[self._played_sentence_count:]
spoken_text = " ".join(spoken) if spoken else "(nothing)"
unspoken_text = " ".join(unspoken) if unspoken else "(nothing)"

annotation = (
    f"[The user interrupted you. "
    f"They heard up to: \"{spoken_text}\". "
    f"Your unspoken response was: \"{unspoken_text}\". "
    f"Adjust based on what the user says next.]"
)

# Send annotation as a user message to the CLI
# (must be done via the event loop since _trigger_barge_in is called from STT stage)
asyncio.create_task(self._send_to_cli(annotation))
print(f"Barge-in: Sent annotation ({self._played_sentence_count}/{len(self._spoken_sentences)} sentences spoken)", flush=True)
```

Note: The annotation is sent as a user message, but the CLI will NOT generate a response to it because there's no subsequent `_read_cli_response()` call — the LLM stage will wait for the next real transcript. When the user's next utterance arrives, the CLI will have the annotation in its context.

Wait — actually `_send_to_cli()` sends a user message, and the CLI in `--input-format stream-json` will generate a response to it. We need to either:
(a) Read and discard the response, or
(b) Send it in a way that doesn't trigger a response

The simplest approach: Don't send annotation as a separate message. Instead, prepend the annotation to the NEXT user message. Store it in `self._barge_in_annotation`:

```python
self._barge_in_annotation = annotation
```

Then in `_llm_stage()`, when building the next user message:
```python
if self._barge_in_annotation:
    user_content = f"{self._barge_in_annotation}\n\n{user_content}"
    self._barge_in_annotation = None
```

This is cleaner — the annotation is delivered with the user's next utterance, giving the AI full context.

So: Add `self._barge_in_annotation = None` in `__init__`, set it in `_trigger_barge_in()`, and consume it in `_llm_stage()`.
  </action>
  <verify>
1. `grep -n "_spoken_sentences" live_session.py` shows tracking in __init__, _read_cli_response, and _trigger_barge_in
2. `grep -n "_played_sentence_count" live_session.py` shows counting in playback stage
3. `grep -n "sentence_done" live_session.py` shows sentinel in TTS stage and handling in playback stage
4. `grep -n "_barge_in_annotation" live_session.py` shows storage, population in trigger, and consumption in LLM stage
5. `python3 -c "import live_session; print('OK')"` — module imports without error
  </verify>
  <done>Sentences sent to TTS are tracked. Playback stage counts completed sentences via sentinel frames. On barge-in, an annotation describing spoken vs unspoken text is stored and prepended to the user's next message, giving the AI context about the interruption.</done>
</task>

<task type="auto">
  <name>Task 2: Shorten post-interrupt silence threshold and add overlay pulse</name>
  <files>live_session.py</files>
  <action>
**Shortened post-interrupt silence threshold in `_stt_stage()`:**

Add state tracking:
- `self._post_barge_in = False` in `__init__` — set True by `_trigger_barge_in()`, cleared after the next transcription completes

In `_trigger_barge_in()`, add: `self._post_barge_in = True`

In `_stt_stage()`, use a dynamic silence duration. Where `SILENCE_DURATION = 0.8` is used for checking silence timeout, make it variable:

Replace the single constant with:
```python
SILENCE_DURATION_NORMAL = 0.8
SILENCE_DURATION_POST_BARGE = 0.4  # Faster response after interruption
```

And in the silence detection check:
```python
current_silence_duration = SILENCE_DURATION_POST_BARGE if self._post_barge_in else SILENCE_DURATION_NORMAL
```

Use `current_silence_duration` instead of `SILENCE_DURATION` in the silence timeout comparison.

After a successful transcription (when a transcript is produced and sent to `_stt_out_q`), clear the flag:
```python
self._post_barge_in = False
```

**Overlay pulse on barge-in:**

In `_trigger_barge_in()`, the status is already set to "listening" (from Plan 01). To create a brief visual pulse, momentarily set a "barge_in" status before transitioning to "listening":

```python
self._set_status("barge_in")
# The overlay will see "barge_in" briefly, then "listening"
# Use a short delay so the pulse is visible
await asyncio.sleep(0.15)
self._set_status("listening")
```

This relies on the indicator reading the status file. The indicator already polls every 100ms, so a 150ms pulse should be visible for 1-2 poll cycles. The indicator's status display logic will need to handle "barge_in" — but since it falls through to a default color, it will just show a different state briefly. For a more visible pulse, we can use the existing status infrastructure.

Actually, per CONTEXT.md: "Brief visual pulse/flash on the overlay when barge-in activates — momentary acknowledgment, then transitions to listening state (no new overlay state)." So we should NOT add a new status state. Instead, just set "listening" directly. The transition from "speaking" to "listening" IS the visual change. The 150ms delay is unnecessary.

Simplify: In `_trigger_barge_in()`, just set `self._set_status("listening")` (already done in Plan 01). No additional overlay work needed — the "speaking" → "listening" transition is the acknowledgment.

**Log the barge-in with context for debugging:**
In `_trigger_barge_in()`, enhance the log event:
```python
self._log_event("barge_in",
    spoken_sentences=self._played_sentence_count,
    total_sentences=len(self._spoken_sentences),
    cooldown_until=self._barge_in_cooldown_until
)
```
  </action>
  <verify>
1. `grep -n "SILENCE_DURATION_POST_BARGE\|SILENCE_DURATION_NORMAL" live_session.py` shows both constants
2. `grep -n "_post_barge_in" live_session.py` shows flag in __init__, trigger, and STT stage
3. `grep -n "current_silence_duration" live_session.py` shows dynamic threshold usage
4. `python3 -c "import live_session; print('OK')"` — module imports without error
  </verify>
  <done>Post-interrupt silence threshold is halved (0.4s vs 0.8s) for one turn after barge-in, so the AI responds faster to quick corrections. Overlay transitions naturally from "speaking" to "listening" on barge-in. Barge-in events are logged with sentence tracking metadata.</done>
</task>

</tasks>

<verification>
1. Module imports: `python3 -c "from live_session import LiveSession; print('OK')"`
2. Sentence tracking: `_spoken_sentences` populated in `_read_cli_response`, consumed in `_trigger_barge_in`
3. Sentence counting: `sentence_done` sentinel in TTS stage, counted in playback stage
4. Annotation: `_barge_in_annotation` stored on barge-in, prepended to next user message in LLM stage
5. Dynamic silence: `SILENCE_DURATION_POST_BARGE` used when `_post_barge_in` is True
6. Full flow: User speaks during AI playback → VAD detects ~0.5s speech → barge-in triggers → queues drain → trailing filler plays → annotation stored → status changes to listening → user speaks → shortened silence threshold → transcription → annotation prepended → AI responds knowing it was interrupted
</verification>

<success_criteria>
- Interrupted AI response text is annotated with spoken vs. unspoken portions
- Annotation is prepended to the user's next message (not sent as a separate turn)
- Post-interrupt silence threshold is 0.4s (vs normal 0.8s) for one turn
- AI can adapt its response based on knowing it was interrupted
- Status transitions cleanly from "speaking" to "listening" on barge-in
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/05-barge-in/05-02-SUMMARY.md`
</output>
