---
phase: 12-infrastructure-safety-net
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - vram_monitor.py
  - test_live_session.py
  - ~/.config/pipewire/pipewire.conf.d/echo-cancel.conf
autonomous: false

must_haves:
  truths:
    - "Whisper distil-large-v3 and Ollama Llama 3.2 3B run simultaneously on the RTX 3070 without OOM"
    - "VRAMMonitor reports usage levels (ok/warning/critical/emergency) from NVML"
    - "PipeWire echo cancellation source exists and pasimple can record from it"
  artifacts:
    - path: "vram_monitor.py"
      provides: "NVML-based GPU memory watchdog with threshold levels"
      contains: "class VRAMMonitor"
    - path: "~/.config/pipewire/pipewire.conf.d/echo-cancel.conf"
      provides: "PipeWire WebRTC echo cancellation module configuration"
      contains: "libpipewire-module-echo-cancel"
  key_links:
    - from: "vram_monitor.py"
      to: "pynvml"
      via: "NVML Python bindings"
      pattern: "pynvml\\.nvml"
---

<objective>
Validate VRAM budget (go/no-go gate), configure PipeWire echo cancellation, and build the VRAMMonitor module.

Purpose: The VRAM validation is the Phase 12 go/no-go gate -- if Whisper distil-large-v3 + Ollama don't fit in 8GB, the entire phase architecture changes. PipeWire AEC prevents the AI from hearing itself. VRAMMonitor enables proactive VRAM management for long-running sessions.

Output: VRAMMonitor module with tests, PipeWire AEC config file, validated VRAM budget confirming the phase can proceed.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-infrastructure-safety-net/12-RESEARCH.md
@.planning/phases/12-infrastructure-safety-net/12-CONTEXT.md
@live_session.py (lines 1306-1391 for VAD pattern, lines 2020-2067 for capture pattern)
@test_live_session.py (test patterns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install nvidia-ml-py, create VRAMMonitor module with tests</name>
  <files>vram_monitor.py, test_live_session.py</files>
  <action>
  1. Install nvidia-ml-py in the service venv:
     ```
     ~/.local/share/push-to-talk/venv/bin/pip install nvidia-ml-py
     ```

  2. Write tests FIRST in test_live_session.py (TDD: RED phase). Add a new test section "VRAMMonitor Tests". Tests should mock pynvml to avoid requiring GPU in test environment:
     - test_vram_monitor_check_ok: mock NVML returning 4000MB used of 8192MB total -> check() returns "ok"
     - test_vram_monitor_check_warning: mock 6500MB used -> returns "warning"
     - test_vram_monitor_check_critical: mock 7200MB used -> returns "critical"
     - test_vram_monitor_check_emergency: mock 7900MB used -> returns "emergency"
     - test_vram_monitor_get_usage_mb: mock 3000MB used -> get_usage_mb() returns 3000
     - test_vram_monitor_get_free_mb: mock 3000MB used of 8192MB -> get_free_mb() returns 5192
     - test_vram_monitor_init_failure_graceful: pynvml.nvmlInit raises exception -> VRAMMonitor.create() returns None (factory method, not crash)

  3. Run tests, confirm they fail (RED).

  4. Create vram_monitor.py (TDD: GREEN phase):
     - class VRAMMonitor with thresholds:
       WARNING_MB = 6144 (75% of 8192)
       CRITICAL_MB = 7168 (87.5%)
       EMERGENCY_MB = 7782 (95%)
     - __init__: call pynvml.nvmlInit(), get handle for GPU 0, store total_mb
     - @classmethod create() -> VRAMMonitor | None: factory that returns None on init failure
     - get_usage_mb() -> int: returns used VRAM in MB
     - get_free_mb() -> int: returns free VRAM in MB
     - check() -> str: returns "ok", "warning", "critical", or "emergency"
     - get_stats() -> dict: returns {"used_mb", "free_mb", "total_mb", "level", "utilization_pct"}
     - shutdown(): calls pynvml.nvmlShutdown()

  5. Run tests, confirm they pass (GREEN).

  Follow existing test patterns from test_live_session.py: use the @test decorator, run_test() function, and mock-based testing. Use unittest.mock.patch to mock pynvml calls.
  </action>
  <verify>
  Run `python3 test_live_session.py` -- all tests pass including new VRAMMonitor tests.
  Run `python3 -c "from vram_monitor import VRAMMonitor; v = VRAMMonitor.create(); print(v.get_stats() if v else 'No GPU')"` -- prints actual GPU stats on this machine.
  </verify>
  <done>VRAMMonitor module exists with 7 tests passing, factory method handles missing GPU gracefully, get_stats() returns structured data.</done>
</task>

<task type="auto">
  <name>Task 2: VRAM validation spike + PipeWire AEC configuration</name>
  <files>~/.config/pipewire/pipewire.conf.d/echo-cancel.conf</files>
  <action>
  **Part A: VRAM Validation (go/no-go gate)**

  This is the critical gate. Run the validation procedure from the research:

  1. Use the VRAMMonitor from Task 1 to get baseline VRAM usage.

  2. Load Whisper distil-large-v3 int8_float16 and measure VRAM:
     ```python
     from faster_whisper import WhisperModel
     m = WhisperModel("distil-large-v3", device="cuda", compute_type="int8_float16")
     ```
     Record VRAM after loading.

  3. In a separate terminal, load Ollama Llama 3.2 3B:
     ```bash
     ollama run llama3.2:3b "Hello, this is a VRAM test" --verbose
     ```
     Record VRAM with both loaded.

  4. Stress test: run a Whisper transcription while Ollama generates text. Record peak VRAM.

  5. Document results. If peak VRAM > 7500MB, the fallback chain from CONTEXT.md applies:
     - Try reducing Ollama num_ctx to 1024
     - If still too high, note that Whisper has priority and Ollama will be unloaded during concurrent use

  Record all measurements in the SUMMARY for this plan.

  **IMPORTANT:** If VRAM validation FAILS (doesn't fit at all), stop and report. This is the go/no-go gate.

  **Part B: PipeWire Echo Cancellation**

  1. Create the PipeWire AEC config file at ~/.config/pipewire/pipewire.conf.d/echo-cancel.conf:
     ```
     context.modules = [
         {   name = libpipewire-module-echo-cancel
             args = {
                 monitor.mode = true
                 capture.props = {
                     node.name = "Echo Cancellation Capture"
                     node.passive = true
                 }
                 source.props = {
                     node.name   = "Echo Cancellation Source"
                     node.description = "Echo-Cancelled Microphone"
                 }
                 playback.props = {
                     node.name = "Echo Cancellation Playback"
                     node.passive = true
                 }
             }
         }
     ]
     ```

  2. Restart PipeWire:
     ```bash
     systemctl --user restart pipewire pipewire-pulse
     ```

  3. Find the exact PulseAudio device name:
     ```bash
     pactl list short sources | grep -i echo
     ```
     Record the exact device name string.

  4. Test recording from the AEC source with pasimple:
     ```python
     import pasimple
     pa = pasimple.PaSimple(
         pasimple.PA_STREAM_RECORD,
         pasimple.PA_SAMPLE_S16LE,
         1, 24000,
         app_name='push-to-talk-test',
         device_name="<exact name from step 3>"
     )
     data = pa.read(4096)
     print(f"Read {len(data)} bytes from AEC source")
     del pa
     ```

  5. If AEC source not found: try variations of the name (underscores, dots). Document the working name. If no name works, document that software fallback will be needed (the existing _spoken_sentences fingerprinting pattern in live_session.py).

  Document the working AEC device name in the SUMMARY.
  </action>
  <verify>
  VRAMMonitor shows both models loaded within 8GB budget.
  `pactl list short sources | grep -i echo` shows the AEC source.
  Python pasimple test reads audio bytes from AEC source without error.
  </verify>
  <done>
  VRAM budget validated with measurements documented.
  PipeWire AEC configured and tested -- exact device name recorded.
  Go/no-go gate passed (or fallback chain documented if marginal).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
  VRAM validation completed -- Whisper distil-large-v3 + Ollama Llama 3.2 3B measured on your RTX 3070.
  PipeWire echo cancellation configured and tested.
  VRAMMonitor module created with full test suite.
  </what-built>
  <how-to-verify>
  1. Check VRAM measurements in the plan summary -- confirm peak usage leaves reasonable headroom (target: < 7GB peak)
  2. Run `pactl list short sources | grep -i echo` to see the AEC source
  3. Speak while playing music and check if the AEC source attenuates the music in the recording
  4. Run `python3 test_live_session.py` to confirm all tests pass
  5. Run `python3 -c "from vram_monitor import VRAMMonitor; v = VRAMMonitor.create(); print(v.get_stats())"` to see live GPU stats
  </how-to-verify>
  <resume-signal>Type "approved" to proceed with Plans 02-03, or describe issues (especially VRAM concerns)</resume-signal>
</task>

</tasks>

<verification>
- VRAMMonitor tests pass: `python3 test_live_session.py 2>&1 | grep -E "PASS|FAIL" | grep -i vram`
- VRAMMonitor works on real GPU: `python3 -c "from vram_monitor import VRAMMonitor; v = VRAMMonitor.create(); print(v.check())"`
- PipeWire AEC source exists: `pactl list short sources | grep -i echo`
- VRAM budget documented with actual measurements in SUMMARY
</verification>

<success_criteria>
- VRAM go/no-go gate passed with documented measurements
- VRAMMonitor module exists and passes all 7 tests
- PipeWire AEC configured with verified device name
- User has approved VRAM headroom as sufficient
</success_criteria>

<output>
After completion, create `.planning/phases/12-infrastructure-safety-net/12-01-SUMMARY.md`

IMPORTANT: The SUMMARY must include:
- Exact VRAM measurements (baseline, Whisper-only, Whisper+Ollama, concurrent inference peak)
- Exact PipeWire AEC device name string for use in Plan 03
- Go/no-go decision with reasoning
</output>
