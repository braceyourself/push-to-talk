---
phase: 09-semantic-matching-pipeline-polish
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - input_classifier.py
  - category_exemplars.json
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Input that matches no keyword pattern (e.g. 'could you take a peek at this') returns correct category via semantic similarity"
    - "Short backchannel input ('yes', 'ok', 'mhm') is flagged as trivial in classifier response"
    - "When ai_asked_question is true, even trivial-looking input is NOT flagged as trivial"
    - "Classifier daemon still starts and signals CLASSIFIER_READY within 3 seconds"
  artifacts:
    - path: "input_classifier.py"
      provides: "Heuristic classifier + model2vec semantic fallback + trivial detection"
      contains: "SemanticFallback"
    - path: "category_exemplars.json"
      provides: "5-10 exemplar phrases per category for semantic similarity matching"
      contains: "task"
    - path: "requirements.txt"
      provides: "Updated dependencies including model2vec and pysbd"
      contains: "model2vec"
  key_links:
    - from: "input_classifier.py"
      to: "category_exemplars.json"
      via: "SemanticFallback.__init__ loads exemplars and pre-computes embeddings"
      pattern: "category_exemplars"
    - from: "input_classifier.py"
      to: "IPC response"
      via: "handle_client adds trivial and match_type fields to JSON response"
      pattern: "trivial"
---

<objective>
Add model2vec semantic fallback and trivial input detection to the classifier daemon.

Purpose: When heuristic regex patterns produce low-confidence results, semantic similarity against category exemplars provides accurate classification for paraphrased/indirect inputs. Trivial backchannel inputs ("yes", "ok") are flagged so the filler manager can suppress clips for them.

Output: Enhanced `input_classifier.py` with SemanticFallback class + is_trivial() function, `category_exemplars.json` with exemplar phrases, updated `requirements.txt`.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@input_classifier.py
@requirements.txt
@.planning/phases/09-semantic-matching-pipeline-polish/09-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add semantic fallback + trivial detection to classifier daemon</name>
  <files>input_classifier.py, category_exemplars.json, requirements.txt</files>
  <action>
  **1. Create `category_exemplars.json`** in the project root with 5-10 diverse exemplar phrases per category. Categories: task, question, conversational, social, emotional, acknowledgment. Include both direct and indirect phrasings. See 09-RESEARCH.md Pattern 5 for example content.

  **2. Add `model2vec` and `pysbd` to `requirements.txt`.**

  **3. Modify `input_classifier.py`:**

  a) Add a `SemanticFallback` class:
     - `__init__(self, exemplars_path: str)`: Load `StaticModel.from_pretrained("minishlab/potion-base-8M")`, read exemplar JSON, pre-compute embeddings for all exemplar phrases per category using `self.model.encode(phrases)`.
     - `classify(self, text: str) -> tuple[str, float]`: Encode input text, compute cosine similarity (`np.dot` + `np.linalg.norm`) against all category exemplar embeddings, return (best_category, normalized_confidence). Normalize raw cosine sim to confidence scale: cosine >= 0.6 maps to 0.8-0.9, 0.4-0.6 maps to 0.5-0.7, < 0.4 maps to 0.2-0.4.

  b) Add trivial input detection:
     - `TRIVIAL_PATTERNS` set: ~30 backchannel phrases ("yes", "yeah", "ok", "mhm", "sure", "got it", "no", "nah", "nope", "ah", "oh", etc.). See 09-RESEARCH.md Pattern 2.
     - `is_trivial(text: str, ai_asked_question: bool = False) -> bool`: If `ai_asked_question` is True, return False. Clean text (lowercase, strip punctuation). If >4 words, return False. If text ends with `?` or matches task patterns, return False. Check against `TRIVIAL_PATTERNS` set.

  c) Update `classify()` function:
     - Accept optional `semantic: SemanticFallback | None` parameter.
     - After heuristic classification, if confidence >= 0.5, return immediately (heuristic is confident).
     - If confidence < 0.5 and semantic model is available, run semantic classify. Higher confidence wins (per CONTEXT.md decision: "when keyword matching and semantic matching disagree, higher confidence wins").
     - Add `match_type` field to `ClassifiedInput` dataclass: "heuristic" or "semantic".

  d) Update IPC protocol in `handle_client()`:
     - Parse `ai_asked_question` from request JSON (default False).
     - Call `is_trivial(text, ai_asked_question)` and include `trivial` boolean in response.
     - Include `match_type` in response.
     - Pass semantic fallback instance to `classify()`.

  e) Update `run_server()` / startup:
     - Load `SemanticFallback` AFTER emitting CLASSIFIER_READY (graceful degradation -- heuristic works while semantic model loads in background).
     - Use a threading approach: emit CLASSIFIER_READY after heuristic patterns compile, then load model2vec in a background thread. Set a flag when semantic model is ready. If semantic not ready, classify falls back to heuristic-only.
     - Print model2vec load status to stderr (not stdout, to avoid confusing CLASSIFIER_READY detection).

  **Important considerations:**
  - The confidence normalization is critical (see 09-RESEARCH.md Pitfall 2): cosine similarity 0.5 != heuristic confidence 0.5.
  - model2vec first-run downloads ~8MB from HuggingFace. Graceful degradation (heuristic-only until model loads) handles this.
  - Keep `TRIVIAL_PATTERNS` as a frozen set for O(1) lookup.
  </action>
  <verify>
  Run the classifier daemon manually and test via socat/netcat:

  ```bash
  python input_classifier.py /tmp/test-classifier.sock &
  # Wait for CLASSIFIER_READY

  # Test semantic fallback (no keyword match for "take a peek")
  echo '{"text": "could you take a peek at this"}' | socat - UNIX-CONNECT:/tmp/test-classifier.sock
  # Should return category: "task" with match_type: "semantic"

  # Test trivial detection
  echo '{"text": "yes", "ai_asked_question": false}' | socat - UNIX-CONNECT:/tmp/test-classifier.sock
  # Should return trivial: true

  # Test trivial override when AI asked question
  echo '{"text": "yes", "ai_asked_question": true}' | socat - UNIX-CONNECT:/tmp/test-classifier.sock
  # Should return trivial: false

  # Test heuristic still wins when confident
  echo '{"text": "fix the bug in the login page"}' | socat - UNIX-CONNECT:/tmp/test-classifier.sock
  # Should return category: "task", match_type: "heuristic"
  ```

  Verify daemon starts and signals readiness within 3 seconds (heuristic-only initially, semantic loads in background).
  </verify>
  <done>
  - Classifier daemon returns semantic fallback classifications for inputs with no keyword match
  - Trivial inputs are flagged in the IPC response
  - ai_asked_question context flag overrides trivial detection
  - Daemon starts within 3 seconds (graceful degradation: heuristic first, semantic loads in background)
  - category_exemplars.json exists with 5-10 phrases per category
  - requirements.txt includes model2vec and pysbd
  </done>
</task>

</tasks>

<verification>
1. Start classifier daemon: `python input_classifier.py /tmp/test.sock` -- CLASSIFIER_READY appears within 3s
2. Send a paraphrased task input -- correct category returned via semantic match
3. Send "mhm" -- trivial: true in response
4. Send "yes" with ai_asked_question: true -- trivial: false
5. Send "fix the bug" -- heuristic match still works, match_type: "heuristic"
6. `category_exemplars.json` is valid JSON with all 6 categories
</verification>

<success_criteria>
Inputs with no keyword pattern match get classified correctly via model2vec semantic similarity. Trivial backchannels are flagged. The classifier daemon starts promptly with graceful degradation.
</success_criteria>

<output>
After completion, create `.planning/phases/09-semantic-matching-pipeline-polish/09-01-SUMMARY.md`
</output>
