---
phase: 09-semantic-matching-pipeline-polish
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - stream_composer.py
  - pipeline_frames.py
autonomous: true

must_haves:
  truths:
    - "StreamComposer accepts filler clips, TTS text sentences, and silence segments through a unified queue"
    - "TTS is generated per-sentence via an async callback, not per-text-block"
    - "Barge-in pauses the composer and returns unplayed segments"
    - "Inter-segment pauses are inserted automatically between sentences and after filler clips"
    - "Pre-buffering starts TTS generation for the next sentence while current segment plays"
  artifacts:
    - path: "stream_composer.py"
      provides: "StreamComposer asyncio class managing unified sentence queue with cadence control"
      contains: "class StreamComposer"
    - path: "pipeline_frames.py"
      provides: "Extended FrameType enum with SENTENCE_DONE type"
      contains: "SENTENCE_DONE"
  key_links:
    - from: "stream_composer.py"
      to: "pipeline_frames.py"
      via: "imports PipelineFrame and FrameType for audio output"
      pattern: "from pipeline_frames import"
    - from: "stream_composer.py"
      to: "audio_out_q"
      via: "StreamComposer._emit_pcm writes frames to the audio output queue"
      pattern: "_audio_out_q.put"
---

<objective>
Create the StreamComposer class that manages a unified audio sentence queue with natural pacing and barge-in awareness.

Purpose: Replace the current separate TTS stage and filler playback paths with a single composer that controls all audio output. This enables smooth clip-to-LLM transitions, natural inter-sentence pauses, and consistent barge-in behavior across all audio segments.

Output: New `stream_composer.py` module with `StreamComposer` class and supporting types. Updated `pipeline_frames.py` with additional frame type.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@pipeline_frames.py
@live_session.py (reference for current _tts_stage, _play_filler_audio, _playback_stage patterns)
@.planning/phases/09-semantic-matching-pipeline-polish/09-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StreamComposer class with unified audio queue</name>
  <files>stream_composer.py, pipeline_frames.py</files>
  <action>
  **1. Update `pipeline_frames.py`:**
  - Add `SENTENCE_DONE = auto()` to FrameType enum (replaces the current CONTROL frame with data="sentence_done" pattern, making it a first-class frame type).

  **2. Create `stream_composer.py`** with the following components:

  a) **SegmentType enum**: FILLER_CLIP, TTS_SENTENCE, SILENCE, NON_SPEECH

  b) **AudioSegment dataclass**:
     - `type: SegmentType`
     - `data: bytes | str` (PCM bytes for clips, text string for TTS sentences)
     - `duration: float` (for SILENCE segments, in seconds)
     - `metadata: dict` (optional, for logging)

  c) **StreamComposer class** (asyncio-based, in-process):

     **Constructor `__init__`:**
     - `audio_out_q: asyncio.Queue` -- the pipeline's audio output queue
     - `tts_fn: Callable[[str], Awaitable[bytes | None]]` -- async function that takes text, returns resampled PCM bytes (or None on error). This is the existing `_tts_to_pcm` style function but made async-compatible.
     - `get_generation_id: Callable[[], int]` -- returns current generation_id for barge-in checking
     - `sample_rate: int = 24000`
     - Internal state: `_segment_q` (asyncio.Queue for AudioSegment | None sentinel), `_current_gen_id`, `_paused` flag, `_held_segments` list, cadence parameters.

     **Cadence parameters** (tunable, stored as instance attributes):
     - `inter_sentence_pause: float = 0.15` (150ms between sentences)
     - `post_clip_pause: float = 0.25` (250ms after filler clip before LLM TTS)
     - `thinking_pause: float = 0.4` (for non-speech thinking sounds)

     **Public API:**
     - `async enqueue(segment: AudioSegment)`: Add segment to queue.
     - `async enqueue_end_of_turn()`: Signal end of turn (None sentinel).
     - `pause() -> list[AudioSegment]`: Barge-in. Set `_paused = True`. Drain remaining queued segments. Return all unplayed segments (held + drained). This is synchronous -- called from barge-in handler.
     - `resume()`: Clear paused state.
     - `reset()`: Clear all state for a new turn (drain queue, clear held, reset counters). Called on generation_id change.

     **Main loop `async run()`:**
     - Loop forever, getting segments from `_segment_q`.
     - On None sentinel: emit END_OF_TURN frame to `_audio_out_q`, continue.
     - If paused: append segment to `_held_segments`, continue.
     - Check `_get_gen_id()` hasn't changed (stale segment detection).
     - **Pre-buffering**: Before processing the current segment, peek at the next segment in the queue. If the next segment is TTS_SENTENCE, start its TTS generation as a background `asyncio.Task` so PCM is ready when needed.
     - Process based on segment type:

       **FILLER_CLIP**: Emit PCM via `_emit_pcm()` as FILLER frames, then emit post_clip_pause silence.

       **TTS_SENTENCE**: If pre-buffered TTS result is available, use it. Otherwise call `_tts_fn()`. Emit PCM as TTS_AUDIO frames. Emit SENTENCE_DONE frame. Emit inter_sentence_pause silence.

       **SILENCE**: Emit silence frames for the specified duration.

       **NON_SPEECH**: Emit PCM as FILLER frames, emit thinking_pause silence.

     **Internal methods:**
     - `async _emit_pcm(pcm: bytes, frame_type: FrameType, gen_id: int)`: Chunk PCM into 4096-byte pieces, write to `_audio_out_q` as PipelineFrame. Check `generation_id` and `_paused` before each chunk write. Return early if either condition is true.
     - `async _emit_silence(duration: float, gen_id: int)`: Generate silence bytes (`b'\x00' * int(sample_rate * 2 * duration)`) and emit via `_emit_pcm`. Skip if duration <= 0.
     - `async _peek_next() -> AudioSegment | None`: Non-blocking peek at the next segment in `_segment_q` (get + put back if not None). Used for pre-buffering lookahead.

  **Design notes:**
  - The composer does NOT replace `_playback_stage`. It sits between content producers (filler manager, LLM response reader) and the existing playback stage. It writes to `_audio_out_q`, playback reads from `_audio_out_q` -- same as today.
  - The `tts_fn` callback is provided by the caller (live_session.py). This keeps TTS implementation details out of the composer. The function should handle Piper subprocess creation, text-to-PCM synthesis, and 22050->24000 resampling internally.
  - Pre-buffering: Use `asyncio.create_task` to start TTS for the next sentence while the current one plays. Store the task as `_prefetch_task`. If the pre-fetched result matches the next segment when it's dequeued, use it; otherwise discard and re-generate.
  - The `pause()` method is intentionally synchronous (no await) because it needs to respond instantly to barge-in events. Queue draining uses `get_nowait()`.
  - `pysbd` is NOT imported here. Sentence splitting happens upstream in `live_session.py` before segments are enqueued. The composer receives one sentence per TTS_SENTENCE segment.
  </action>
  <verify>
  Create a small test script that:
  1. Instantiates StreamComposer with a mock `tts_fn` (returns dummy PCM bytes) and mock `get_generation_id` (returns constant)
  2. Enqueues a FILLER_CLIP segment, two TTS_SENTENCE segments, and an end-of-turn sentinel
  3. Runs the composer for a few seconds
  4. Verifies frames appear in audio_out_q in correct order: FILLER frames, silence, TTS_AUDIO frames, SENTENCE_DONE, silence, TTS_AUDIO frames, SENTENCE_DONE, silence, END_OF_TURN
  5. Tests barge-in: enqueue 5 sentences, call pause() after 1 plays, verify held segments returned

  ```bash
  python -c "
  import asyncio
  from stream_composer import StreamComposer, AudioSegment, SegmentType
  from pipeline_frames import PipelineFrame, FrameType

  async def test():
      q = asyncio.Queue()
      async def mock_tts(text):
          return b'\\x00' * 4800  # 100ms of silence at 24kHz 16-bit
      composer = StreamComposer(q, mock_tts, lambda: 1)
      task = asyncio.create_task(composer.run())

      # Enqueue filler + 2 sentences + end
      await composer.enqueue(AudioSegment(SegmentType.FILLER_CLIP, data=b'\\x00' * 2400))
      await composer.enqueue(AudioSegment(SegmentType.TTS_SENTENCE, data='Hello there.'))
      await composer.enqueue(AudioSegment(SegmentType.TTS_SENTENCE, data='How are you?'))
      await composer.enqueue_end_of_turn()

      # Collect frames
      frames = []
      for _ in range(100):
          try:
              f = q.get_nowait()
              frames.append(f)
          except asyncio.QueueEmpty:
              await asyncio.sleep(0.05)
              continue

      types = [f.type for f in frames]
      assert FrameType.FILLER in types, 'Missing FILLER frames'
      assert FrameType.TTS_AUDIO in types, 'Missing TTS_AUDIO frames'
      assert FrameType.SENTENCE_DONE in types, 'Missing SENTENCE_DONE frames'
      assert FrameType.END_OF_TURN in types, 'Missing END_OF_TURN frame'
      print('All checks passed')
      task.cancel()

  asyncio.run(test())
  "
  ```
  </verify>
  <done>
  - StreamComposer class exists in stream_composer.py with enqueue/pause/resume/reset/run API
  - FrameType.SENTENCE_DONE added to pipeline_frames.py
  - Filler clips, TTS sentences, and silence segments flow through a unified queue
  - Pre-buffering starts next sentence's TTS while current segment plays
  - Barge-in (pause) immediately stops output and returns unplayed segments
  - Inter-segment pauses are inserted automatically
  </done>
</task>

</tasks>

<verification>
1. `stream_composer.py` exists and imports cleanly: `python -c "from stream_composer import StreamComposer"`
2. `pipeline_frames.py` has SENTENCE_DONE: `python -c "from pipeline_frames import FrameType; print(FrameType.SENTENCE_DONE)"`
3. Test script verifies correct frame ordering through the composer
4. Pause/resume API works: segments are held during pause, released on resume
</verification>

<success_criteria>
StreamComposer manages a unified audio sentence queue with TTS-per-sentence generation, automatic inter-segment pauses, pre-buffering, and barge-in awareness. The class is ready for integration into live_session.py.
</success_criteria>

<output>
After completion, create `.planning/phases/09-semantic-matching-pipeline-polish/09-02-SUMMARY.md`
</output>
