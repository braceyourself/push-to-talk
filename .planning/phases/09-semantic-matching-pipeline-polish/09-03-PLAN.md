---
phase: 09-semantic-matching-pipeline-polish
plan: 03
type: execute
wave: 2
depends_on: ["09-01", "09-02"]
files_modified:
  - live_session.py
autonomous: false

must_haves:
  truths:
    - "User says 'could you take a peek at this' (no keyword match) and hears a task-appropriate response clip"
    - "User says 'yes' or 'mhm' and hears natural silence instead of a filler clip"
    - "User barges in during LLM TTS and the current sentence finishes, then playback pauses cleanly"
    - "Quick response clip transitions smoothly to LLM TTS with a brief natural pause and no audio gap or overlap"
    - "ai_asked_question flag is set when AI's last sentence ends with ? or contains question patterns"
  artifacts:
    - path: "live_session.py"
      provides: "Full pipeline integration: composer replaces direct TTS/filler writes, trivial detection suppresses fillers, barge-in interacts with composer"
      contains: "StreamComposer"
  key_links:
    - from: "live_session.py"
      to: "stream_composer.py"
      via: "self._composer = StreamComposer(...) in run(), composer.enqueue() in _filler_manager and _read_cli_response"
      pattern: "_composer"
    - from: "live_session.py"
      to: "input_classifier.py"
      via: "_classify_input sends ai_asked_question flag, reads trivial flag from response"
      pattern: "ai_asked_question"
    - from: "live_session.py _trigger_barge_in"
      to: "stream_composer.py"
      via: "self._composer.pause() called during barge-in to stop audio output"
      pattern: "composer.pause"
    - from: "live_session.py _read_cli_response"
      to: "stream_composer.py"
      via: "Sentences enqueued as TTS_SENTENCE segments instead of written to _llm_out_q"
      pattern: "composer.enqueue.*TTS_SENTENCE"
---

<objective>
Wire the classifier enhancements (semantic fallback, trivial detection) and stream composer into the live session pipeline.

Purpose: Connect all Phase 9 components into the running pipeline so users experience semantic-aware classification, trivial input silence, smooth clip-to-LLM transitions, and clean barge-in behavior.

Output: Updated `live_session.py` with composer-based audio pipeline replacing the current direct TTS stage + filler playback paths.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@live_session.py
@stream_composer.py
@input_classifier.py
@.planning/phases/09-semantic-matching-pipeline-polish/09-01-SUMMARY.md
@.planning/phases/09-semantic-matching-pipeline-polish/09-02-SUMMARY.md
@.planning/phases/09-semantic-matching-pipeline-polish/09-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate composer + classifier enhancements into pipeline</name>
  <files>live_session.py</files>
  <action>
  This task makes 5 targeted modifications to `live_session.py`:

  **1. Initialize StreamComposer in `run()`:**
  - Import `StreamComposer, AudioSegment, SegmentType` from `stream_composer`.
  - After creating pipeline queues, create a TTS callback function that wraps the existing `_tts_to_pcm()` method (it already handles Piper + resampling).
  - Instantiate `self._composer = StreamComposer(self._audio_out_q, tts_callback, lambda: self.generation_id)`.
  - Add `self._composer.run()` to the `stages` list in `asyncio.gather()` (replaces `self._tts_stage()`).
  - Remove `self._tts_stage()` from the stages list -- the composer now handles TTS.
  - Add `self._ai_asked_question = False` flag to session state.

  **2. Update `_filler_manager()` to use trivial detection + composer:**
  - Pass `ai_asked_question` flag in the classification request: `{"text": user_text, "ai_asked_question": self._ai_asked_question}`.
  - Read `trivial` flag from classification response.
  - If `trivial` is True: log the classification (with `trivial=True`), set a brief "thinking" status flash, and return immediately (no filler clip, natural silence).
  - When playing a clip: instead of calling `self._play_filler_audio(clip_pcm, cancel_event)`, enqueue it to the composer: `await self._composer.enqueue(AudioSegment(SegmentType.FILLER_CLIP, data=clip_pcm))`. The cancel_event for the 500ms gate still works as before (it gates whether to play at all), but once the decision to play is made, audio goes through the composer.
  - Reset `self._ai_asked_question = False` after classification (user has responded, clear the flag).

  **3. Update `_read_cli_response()` to route sentences through composer:**
  - Import `pysbd` and create a `segmenter = pysbd.Segmenter(language="en", clean=False)` (once, at module level or in `__init__`).
  - In the normal (non-tool-use) streaming path: keep the existing `SENTENCE_END_RE` for incremental sentence detection during streaming (it works well for detecting sentence boundaries as text arrives incrementally).
  - Change what happens when a sentence is detected: instead of putting a TEXT_DELTA frame on `_llm_out_q`, enqueue it as a TTS_SENTENCE segment to the composer: `await self._composer.enqueue(AudioSegment(SegmentType.TTS_SENTENCE, data=clean))`.
  - For post-tool buffer flush and remaining sentence flush at end of turn: use `pysbd` segmenter to split the accumulated text into sentences, then enqueue each as TTS_SENTENCE.
  - After all sentences are enqueued, call `await self._composer.enqueue_end_of_turn()` instead of putting END_OF_TURN on `_llm_out_q`.
  - Track `self._ai_asked_question`: after the full response is accumulated, check if the last sentence ends with `?` or contains question patterns ("should I", "do you want", "would you like", "shall I", "can I"). If so, set `self._ai_asked_question = True`.
  - Keep `self._spoken_sentences` tracking for barge-in annotation (append each sentence text as it's enqueued to composer).

  **4. Update `_trigger_barge_in()` to use composer:**
  - After incrementing `generation_id`, call `held = self._composer.pause()`.
  - The held segments represent unspoken sentences -- use their count/content for the barge-in annotation instead of (or in addition to) the `_spoken_sentences` / `_played_sentence_count` tracking.
  - Call `self._composer.reset()` to clear the composer's internal state for the new generation.
  - Keep the existing trailing acknowledgment clip behavior (queue a brief faded clip to `_audio_out_q` directly -- this is a one-shot trail, not routed through composer).
  - Drain `_audio_out_q` as before (stale frames from previous generation).

  **5. Remove or simplify `_tts_stage()`:**
  - The `_tts_stage()` method is no longer needed because the composer handles TTS generation via its `tts_fn` callback.
  - Remove it from the class and from the `stages` list in `run()`.
  - The `_llm_out_q` queue can be removed from initialization since sentences now go directly from `_read_cli_response` to the composer. However, if other code references `_llm_out_q` (check for drain operations in `_trigger_barge_in` and `_check_interrupt`), update those references. The drain operations should drain the composer's segment queue instead, or just call `self._composer.reset()`.

  **Important considerations:**
  - The `_play_filler_audio()` method can remain for the barge-in trailing clip (step 9 of `_trigger_barge_in`), which writes directly to `_audio_out_q` bypassing the composer intentionally.
  - The `_play_gated_ack()` method (for tool-use acknowledgments) should also route through the composer for consistency: enqueue the ack clip as a FILLER_CLIP segment.
  - Keep `_filler_cancel` and `_ack_cancel` events -- they gate whether to play, but the actual audio flows through the composer.
  - The `SENTENCE_END_RE` regex stays for incremental streaming detection. `pysbd` is used for final flush of post-tool buffer and remaining text at end of turn.
  </action>
  <verify>
  Start a live session and verify:

  1. Session starts without errors (composer runs as pipeline stage)
  2. Say a normal question -- hear a question-appropriate filler clip, then smooth transition to LLM TTS response
  3. Say "yes" or "ok" -- no filler clip plays (trivial detection), but LLM still processes and responds
  4. Say something indirect like "take a look at the readme" -- semantic matching classifies as task
  5. Barge in during LLM response -- current sentence finishes, then stops cleanly
  6. Check session log for classification entries with `trivial` and `match_type` fields

  ```bash
  # Quick smoke test: start session, check for errors in output
  # Look for: "Live session: Pipeline started" with composer stage
  # Look for: classification log entries with match_type field
  ```
  </verify>
  <done>
  - StreamComposer runs as a pipeline stage, replacing _tts_stage
  - Filler clips and LLM TTS sentences both flow through the composer
  - Trivial inputs ("yes", "ok") get natural silence instead of filler clips
  - ai_asked_question flag prevents trivial classification after AI asks a question
  - Barge-in calls composer.pause() for clean sentence-boundary interruption
  - Smooth clip-to-LLM transition with natural pause (no gap, no overlap)
  - pysbd used for post-tool-buffer sentence splitting
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Full Phase 9 integration: semantic matching, trivial detection, stream composer pipeline, barge-in with composer</what-built>
  <how-to-verify>
  Start a live session and test these scenarios:

  1. **Semantic matching**: Say something indirect that has no keyword match, like "could you take a peek at the readme" or "have a look at that file". Verify you hear a task-appropriate filler (not a generic acknowledgment).

  2. **Trivial silence**: Say just "yes" or "ok" or "mhm" after the AI gives a statement (not a question). Verify NO filler clip plays -- just natural silence before the AI responds.

  3. **Question context override**: Wait for the AI to ask you a question (ending with ?). Then say "yes". Verify a filler DOES play (because the AI asked a question, "yes" is a real answer, not trivial).

  4. **Smooth transition**: Say any question or command. Listen for: filler clip -> brief natural pause -> LLM TTS response. The transition should feel like one continuous speaker, not two separate audio events.

  5. **Barge-in**: Say something that triggers a long response. While the AI is speaking, start talking. Verify the AI finishes its current sentence then stops -- it should feel like a natural pause to listen, not a hard cut.

  Type "approved" if all 5 scenarios work, or describe any issues.
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. Live session starts with composer as pipeline stage (no _tts_stage)
2. Classification requests include ai_asked_question flag
3. Trivial inputs produce no filler audio
4. Non-keyword inputs get classified via semantic matching
5. Filler clips transition smoothly to LLM TTS through composer
6. Barge-in interacts with composer.pause() for clean interruption
7. Session JSONL log shows match_type and trivial fields in classification entries
</verification>

<success_criteria>
All four Phase 9 success criteria are met:
1. Paraphrased/indirect inputs get correct classification via semantic matching
2. Trivial inputs ("yes", "ok") get natural silence instead of filler
3. Barge-in during clips/TTS stops cleanly at sentence boundary
4. Clip-to-LLM transition is smooth with natural pause, no artifacts
</success_criteria>

<output>
After completion, create `.planning/phases/09-semantic-matching-pipeline-polish/09-03-SUMMARY.md`
</output>
