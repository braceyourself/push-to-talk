---
phase: 04-filler-system-overhaul
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - clip_factory.py
  - audio/fillers/nonverbal/
  - audio/fillers/pool.json
autonomous: true

must_haves:
  truths:
    - "clip_factory.py generates non-verbal WAV clips via Piper TTS CLI"
    - "Generated clips are evaluated for duration, RMS energy, clipping, and silence before acceptance"
    - "The clip pool is capped at a configurable size and oldest clips are rotated out when exceeded"
    - "Pool metadata (filename, quality scores, creation date, params) is persisted in pool.json"
    - "Running clip_factory.py seeds the nonverbal/ directory with at least MIN_POOL_SIZE clips"
  artifacts:
    - path: "clip_factory.py"
      provides: "Background daemon for non-verbal clip generation, evaluation, and pool rotation"
      min_lines: 120
    - path: "audio/fillers/nonverbal/"
      provides: "Directory of generated non-verbal WAV clips"
    - path: "audio/fillers/pool.json"
      provides: "Metadata tracking for clip pool (quality, age, params)"
  key_links:
    - from: "clip_factory.py"
      to: "Piper TTS CLI"
      via: "subprocess.run with --output-raw"
      pattern: "subprocess\\.run.*piper.*--output-raw"
    - from: "clip_factory.py"
      to: "audio/fillers/pool.json"
      via: "JSON read/write for metadata persistence"
      pattern: "json\\.(loads|dumps).*pool"
---

<objective>
Create the clip factory daemon that generates, evaluates, and manages a rotating pool of non-verbal filler audio clips via Piper TTS.

Purpose: The clip factory is the production system for non-verbal filler clips (hums, breaths). It replaces the old one-time `generate_fillers.py` script (which used OpenAI TTS for verbal clips) with a background daemon that can be spawned at session start to maintain a fresh, diverse pool.

Output: A working `clip_factory.py` script and a seeded `audio/fillers/nonverbal/` directory with clips ready for playback.
</objective>

<execution_context>
@/home/ethan/.claude/get-shit-done/workflows/execute-plan.md
@/home/ethan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-filler-system-overhaul/04-RESEARCH.md

Key codebase references:
- @learner.py (subprocess daemon pattern to follow — Popen with start_new_session=True, CLI arg for session path)
- @live_session.py lines 44-46 (PIPER_CMD, PIPER_MODEL, PIPER_SAMPLE_RATE constants)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create clip_factory.py daemon with generation, evaluation, and rotation</name>
  <files>clip_factory.py</files>
  <action>
Create `clip_factory.py` in the project root (same level as `learner.py`). This is a standalone Python script that:

1. **Constants and config** at module level:
   - `PIPER_CMD` and `PIPER_MODEL` — same paths as live_session.py (use Path.home() / ".local/share/push-to-talk/..." pattern)
   - `CLIP_DIR = Path(__file__).parent / "audio" / "fillers" / "nonverbal"`
   - `POOL_META = Path(__file__).parent / "audio" / "fillers" / "pool.json"`
   - `POOL_SIZE_CAP = 20` (configurable max clips)
   - `MIN_POOL_SIZE = 10` (generate until reaching this)
   - `SAMPLE_RATE = 22050` (Piper native rate)
   - `PROMPTS = ["Hmm", "Mmm", "Mhm", "Hm", "Mmhmm", "Hmmm", "Ahh", "Uhh"]`

2. **`random_synthesis_params()`** — returns dict with random prompt (from PROMPTS), length_scale (0.7-1.8), noise_w_scale (0.3-1.5), noise_scale (0.4-1.0). Use `random.uniform()` and `round(..., 2)`.

3. **`generate_clip(prompt, length_scale, noise_w, noise_scale)`** — runs Piper via `subprocess.run()` (blocking, NOT asyncio) with `--output-raw`, `--length-scale`, `--noise-w-scale`, `--noise-scale` flags. Input is `prompt.encode()` via stdin. Returns raw PCM bytes or None on failure. Timeout of 10s.

4. **`evaluate_clip(pcm_data)`** — uses numpy to compute duration, RMS energy, peak, clipping_ratio (samples >= 32000), silence_ratio (samples < 500). Returns dict with all scores plus a `"pass"` boolean: duration 0.2-2.0s AND rms > 300 AND clipping < 0.01 AND silence < 0.7.

5. **`save_clip(pcm_data, params, scores)`** — writes PCM to a WAV file in CLIP_DIR with filename pattern `{prompt}_{NNN}.wav` (NNN is zero-padded sequential). Uses stdlib `wave` module (1ch, 2 bytes, 22050Hz). Returns the filename.

6. **`load_pool_meta()`** — reads pool.json, returns list of dicts. Returns empty list if file missing.

7. **`save_pool_meta(meta)`** — writes list of dicts to pool.json with indent=2.

8. **`rotate_pool(meta)`** — if `len(meta) > POOL_SIZE_CAP`, sort by `created_at` ascending, remove oldest entries until at cap. Delete the corresponding WAV files from disk. Return updated meta.

9. **`top_up_pool()`** — the main logic:
   - Ensure CLIP_DIR exists (mkdir -p)
   - Load pool meta
   - While len(meta) < MIN_POOL_SIZE:
     - Generate random params
     - Generate clip
     - If None, continue (log and retry)
     - Evaluate clip
     - If not passing, log rejection reason and continue
     - Save clip to WAV
     - Append entry to meta: filename, created_at (time.time()), params dict, scores dict
     - Log success
   - Rotate pool (in case pre-existing clips pushed over cap)
   - Save pool meta
   - Print summary: "Clip factory: pool at {N} clips (cap {POOL_SIZE_CAP})"

10. **`daemon_mode(check_interval=300)`** — infinite loop: `top_up_pool()`, then `time.sleep(check_interval)`. For long sessions, periodically ensures pool stays healthy.

11. **`if __name__ == "__main__"` block:**
    - Accept optional `--daemon` flag (runs daemon_mode)
    - Accept optional `--interval N` (daemon check interval, default 300)
    - Without --daemon: run `top_up_pool()` once and exit
    - Use argparse for CLI args

Follow the learner.py pattern: standalone script, uses sys.executable awareness, clean error handling with try/except around generation calls, prints status to stdout.

Do NOT use asyncio anywhere — this is a synchronous subprocess script.
  </action>
  <verify>
Run: `cd /home/ethan/code/push-to-talk && python clip_factory.py`

Expected: Script generates clips, prints progress, creates `audio/fillers/nonverbal/` directory with WAV files, creates `audio/fillers/pool.json` with metadata entries. No errors.

Also verify: `python -c "import clip_factory; print(clip_factory.evaluate_clip(b'\\x00' * 44100))"` should return a dict with `"pass": False` (silence).
  </verify>
  <done>
clip_factory.py exists, is executable, generates at least MIN_POOL_SIZE non-verbal clips on first run, pool.json contains valid metadata for each clip, evaluation rejects bad clips (too short, silent, clipped).
  </done>
</task>

<task type="auto">
  <name>Task 2: Seed the nonverbal clip pool and verify clip quality</name>
  <files>audio/fillers/nonverbal/, audio/fillers/pool.json</files>
  <action>
Run the clip factory to seed the initial pool:

```bash
cd /home/ethan/code/push-to-talk
python clip_factory.py
```

After generation, verify the output:
1. Check that `audio/fillers/nonverbal/` contains at least 10 WAV files
2. Check that `audio/fillers/pool.json` exists and has matching entries
3. Spot-check a few clips: read with `wave` module, verify they are 22050Hz mono 16-bit, duration between 0.2s and 2.0s
4. Verify no clip has mostly silence (play a couple if possible, or check RMS > 300)

If any clips failed evaluation, the factory should have already rejected them. If the pool has fewer than MIN_POOL_SIZE clips after the run, there may be an issue with Piper parameters — adjust the parameter ranges in clip_factory.py and re-run.

This task ensures we have a working pool of clips ready for Plan 02 to wire into the live session.
  </action>
  <verify>
```bash
python -c "
import json
from pathlib import Path
pool = json.loads(Path('audio/fillers/pool.json').read_text())
wavs = list(Path('audio/fillers/nonverbal').glob('*.wav'))
print(f'Pool entries: {len(pool)}')
print(f'WAV files: {len(wavs)}')
assert len(pool) >= 10, 'Pool too small'
assert len(wavs) >= 10, 'Not enough WAV files'
assert all(p.get('scores', {}).get('pass') for p in pool), 'Some clips did not pass evaluation'
print('All checks passed')
"
```
  </verify>
  <done>
audio/fillers/nonverbal/ contains at least 10 WAV files. pool.json has matching metadata with all clips passing quality evaluation. Clips are 22050Hz mono 16-bit PCM with durations 0.2-2.0s.
  </done>
</task>

</tasks>

<verification>
1. `clip_factory.py` exists in project root and runs without errors
2. `python clip_factory.py` produces clips in `audio/fillers/nonverbal/`
3. `pool.json` has valid metadata for all generated clips
4. All pool entries have `scores.pass == True`
5. WAV files are valid 22050Hz mono 16-bit audio
6. `python clip_factory.py` is idempotent — running again does not duplicate clips (pool stays at cap)
</verification>

<success_criteria>
- clip_factory.py can generate, evaluate, and save non-verbal clips via Piper TTS
- Pool rotation works (exceeding cap removes oldest clips)
- Quality evaluation rejects bad clips (silence, clipping, wrong duration)
- At least 10 clips seeded and ready for live session use
</success_criteria>

<output>
After completion, create `.planning/phases/04-filler-system-overhaul/04-01-SUMMARY.md`
</output>
